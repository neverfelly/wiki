<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>neverfelly&#39;s Wiki</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wiki.rechinx.top/"/>
  <updated>2020-11-25T02:34:11.301Z</updated>
  <id>http://wiki.rechinx.top/</id>
  
  <author>
    <name>neverfelly</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="http://wiki.rechinx.top/wiki/hello-world/"/>
    <id>http://wiki.rechinx.top/wiki/hello-world/</id>
    <published>2020-11-25T02:34:11.301Z</published>
    <updated>2020-11-25T02:34:11.301Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LightEnhancement</title>
    <link href="http://wiki.rechinx.top/wiki/ComputerScience/ComputerVision/LightEnhancement/"/>
    <id>http://wiki.rechinx.top/wiki/ComputerScience/ComputerVision/LightEnhancement/</id>
    <published>2019-08-25T11:30:16.000Z</published>
    <updated>2020-11-25T02:34:11.297Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Traditional"><a href="#Traditional" class="headerlink" title="Traditional"></a>Traditional</h2><h3 id="Retinex"><a href="#Retinex" class="headerlink" title="Retinex"></a>Retinex</h3><ul><li>tranditional vision</li><li>Based on equation $S(x, y)=R(x,y)L(x,y)$</li><li>$r(x,y)=\log S(x,y)-\log [F(x,y)\ast S(x,y)]$ ,$\ast$ denote convolution，$F(x,y)=\lambda e^{-(x^2+y^2)\over c^2}$, which must statisify $\int \int F(x,y)dxdy=1$</li><li><ul><li><p>SSR 算法流程：<img src="./ssr.png" alt="image-20190905214238128"></p></li><li><p>MSR:$r(x,y)=\sum_{k}w_k(\log S(x,y)-\log F_k(x,y)\cdot S(x,y))$ </p></li><li><p>LIME：1.Initial Illumination map estimation <script type="math/tex">\textbf{L}={\textbf{S}-1+a\over 1-{1\over a}+\max_{c}{\textbf{S}^c \over a}+\epsilon} + (1-a)</script>  2. Illumination map refinement  <script type="math/tex">\min _{\mathbf{T}}\|\hat{\mathbf{T}}-\mathbf{T}\|_{F}^{2}+\alpha \sum_{x} \frac{\mathbf{W}_{h}(x)\left(\nabla_{h} \mathbf{T}(x)\right)^{2}}{\left|\nabla_{h} \hat{\mathbf{T}}(x)\right|+\epsilon}+\frac{\mathbf{W}_{v}(x)\left(\nabla_{v} \mathbf{T}(x)\right)^{2}}{\left|\nabla_{v} \hat{\mathbf{T}}(x)\right|+\epsilon}</script> conclusion: best traditional method</p></li></ul></li><li>Retinex based methods suffer from the limitation in model capacity of the decomposition for reflectance and illumination</li></ul><h3 id="Gradient-based"><a href="#Gradient-based" class="headerlink" title="Gradient based"></a>Gradient based</h3><ul><li><p>Advantage : simultaneously apply the gradient-based filtering.</p></li><li><script type="math/tex; mode=display">q_h(\boldsymbol{x})=f_h(\boldsymbol{x})\cdot L(f(x);\beta, \tau)</script><script type="math/tex; mode=display">L(\xi; \beta, \tau)=\begin{cases} {\beta -1\over 2\tau^2}\xi^2-{\beta-1\over\tau}\xi+\beta & (\xi \le \tau) \\ 1 & (\xi>\tau)\end{cases}</script><p>$f(\boldsymbol{x})$ and $\xi$ denote pixel intensity, $L$ is enhancement function, $q$ denote enhanced pixel</p></li><li><p>could do optional gradient filtering</p></li><li><p>don’t know how to code</p></li></ul><h2 id="DeepMethods"><a href="#DeepMethods" class="headerlink" title="DeepMethods"></a>DeepMethods</h2><h3 id="MRS-net"><a href="#MRS-net" class="headerlink" title="MRS-net"></a>MRS-net</h3><ul><li><p>Use neural network to process MSR theory, avoid to artificially setting parameters.</p></li><li><p>Multi-scale retinex is a feedforward CNN with different kernels</p><p>For reduction:</p><script type="math/tex; mode=display">r_{MSR_i}(x,y)=\log I_i(x,y)-{1\over 3}\log I_i(x,y)\ast [\sum_{n=1}^{3}K_n e^{-{x^2+y^2\over2c_n^2}}]</script><p>Last summation shows that it aims to summarize guassian functions with variance $c_1,c_2,c_3$. And we know that convolution of two gaussian functions is still a gaussian function, so we can reformat it to a cascading structure with three gaussian function convolotion $c_1,c_2-c_1,c_3-c_2$, and concate each level  at last.</p></li><li><p>Three components: multi=scale loogarithmic transformation: use logarithmic transformation to perform origin image, $n$ procssed images to concate and put into a convolutional layer; difference-of-convolution;color restoration function.</p></li><li><p>Loss: frobenius norm</p></li></ul><h3 id="Retinex-Net"><a href="#Retinex-Net" class="headerlink" title="Retinex-Net"></a>Retinex-Net</h3><ul><li>Three stages: decomposition, enhancement, reconstruction</li><li>decomposition: Data driven by nerual network， input: paired normal light and low light, key: low-light image and normal-light image share the same reflectance. </li><li>loss: <script type="math/tex">\mathcal{L}=\mathcal{L_{recon}}+\mathcal{\lambda_{r}L_r}+\lambda_{s}\mathcal{L_s}</script>, <script type="math/tex">\mathcal{L_{recon}}=\sum_i\sum_j\lambda_{ij}\vert\vert R_i \circ I_j-S_j\vert\vert_1, i \in （low,normal), j \in (low,normal)</script>, <script type="math/tex">\mathcal{L_r}=||R_{low}-R_{normal}||_1</script> , <strong>novel </strong> improved structure-blinding TV loss(Total variation minimizati) to Structure-Aware Smoothness Loss <script type="math/tex">\mathcal{L_s}=\sum_i||\nabla I_i \circ \exp (-\lambda_g \nabla R_i))||</script> , compared with LIME: LIME is weighted by an inital illumination map estimation, rather it is weighted by reflectance.</li><li>Enhancement: encoder-decoder architecture, <strong>multi-scale concatenation</strong> : each resize by nearest-neighbor interpolation to final scale, concatenate to $C \times M$ channel features map, then by convolutional layer reduced and reconstruct the illumination map $\tilde{I}$. Resized-convolutional layer in up-sampling block used for avoid checkerboard pattern of artifacts.</li><li>Loss: <script type="math/tex">\mathcal{L}=\mathcal{L_{recon}}+\mathcal{L_s}, \mathcal{L_{recon}}=||R_{low}\circ \hat{I}-S_{normal}||_1</script> </li><li>Denoising operated on reflectance</li></ul><h3 id="Learning-to-see-in-the-dark"><a href="#Learning-to-see-in-the-dark" class="headerlink" title="Learning to see in the dark"></a>Learning to see in the dark</h3><ul><li>See-in-the-Dark (SID) dataset contains low-light images with paired ground truth.</li><li>what’s on: video capture, denosing, complexity compared to burst image pipeline(graphics TOG 16’)</li><li>Adopt pure FCN: based on recent works that pure FCN can effectively represent many image processing algorithms.</li><li>process sensor data directly</li><li>Context aggregation network and U-Net, not resnet</li><li>Metrics: A/B tests with amazon workers , PSNR/SSIM in controlled experiments for image quality</li><li>Loss, network architecture and data arrangement are tested in controlled experiments: shown that default pipeline outperforme than others</li></ul><h3 id="Kindling-the-Darkness-A-Practical-Low-light-image-enhancer-KinD"><a href="#Kindling-the-Darkness-A-Practical-Low-light-image-enhancer-KinD" class="headerlink" title="Kindling the Darkness: A Practical Low-light image enhancer(KinD)"></a>Kindling the Darkness: A Practical Low-light image enhancer(KinD)</h3><ul><li><p>Three challenges in low-light image enhancement:</p><ul><li>Estimate the illumination component from a single image and adjust light levels</li><li>Remove degradations after light up dark regions</li><li>Train a model without well-defined ground-truth light conditions</li></ul></li><li><p>Contributions: </p><ul><li>trained with paired images captured under different light/exposure conditions, instead of using any ground-truth reflectance and illumination information.</li><li>flexibly adjust light levels</li><li>a module for denosing</li><li>State-of-the-art</li></ul></li><li><p>Layer decomposition: </p><ul><li>learn a mapping function from real data for light level adjustment</li><li>mutual consistency(edge in $\mathbf{I}$, the penalty on $\mathbf{L}$ is small, for a location in a flat, the penalty turns to be large, so it’s edge awared ): <script type="math/tex">\mathcal{L_{mc}}=||\mathbf{M}\circ \exp(-c \cdot \mathbf{M})||_1，\mathbf{M}=|\nabla\mathbf{L}_l|+|\nabla \mathbf{L}_h|</script></li></ul></li><li><p>Reflectance restoration</p><script type="math/tex; mode=display">\mathcal{L}=||\hat{\mathbf{R}}-\mathbf{R}||_2^2-\text{SSIM}(\hat{\mathbf{R}},\mathbf{R_h})+||\nabla\mathbf{\hat{R}}-\nabla\mathbf{R}_h||_2^2</script><p>(question: why can do it? how about previous equation <script type="math/tex">\mathbf{I}=\mathbf{R\circ I}+\mathbf{\hat{E}\circ I}</script>)</p></li><li><p>Illumination adjustment</p><p>Two paramaters for adjustment $\alpha=\mathbf{L_t\over L_s}$,$\gamma=\mathbf{||\log{\hat{L}}||_1\over||\log{L_s}||_1}$ for gamma correction(How to use)</p></li><li><p>Metrics: PSNR,SSIM,LOE,NIQE</p></li></ul><h3 id="Low-light-image-enhancement-algorithm-based-on-retinex-and-generative-adversarial-network-Retinex-GAN"><a href="#Low-light-image-enhancement-algorithm-based-on-retinex-and-generative-adversarial-network-Retinex-GAN" class="headerlink" title="Low-light image enhancement algorithm based on retinex and generative adversarial network(Retinex-GAN)"></a>Low-light image enhancement algorithm based on retinex and generative adversarial network(Retinex-GAN)</h3><ul><li><p>loss</p><p>Regular loss: <script type="math/tex">\mathcal{L_{reg}}={1\over mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}{1\over C-f(R(i,j))}</script>, prevent the illumination image from approaching 1 or -1 to avoid that the network falls into a local optimal solution.</p><p>multi-task loss: <script type="math/tex">\mathcal{L}=\lambda_{rec}\mathcal{L_{rec}}+\lambda_{dec}\mathcal{L_{dec}}+\lambda_{com}\mathcal{L_{com}}+\lambda_{cGAN}\mathcal{L_{cGAN}}</script> </p><p>Smooth L1 loss: <script type="math/tex">\mathcal{L_{L_1}}=smooth_{\mathcal{L_1}}(x)=\begin{cases} 0.5x^2 & if \quad x < 1 \\ x-0.5 & otherwise \end{cases}</script></p><p>reconstruction loss: <script type="math/tex">\mathcal{L_{rec}}=\mathcal{L_{rec_x}}+\mathcal{L_{rec_y}}+\mathcal{L_{reg}}</script></p><p>decomposition loss: <script type="math/tex">\mathcal{L_{dec}}=\mathcal{L_{L_1}}(I_x,I_y)</script> make the image in different brightness is decomposed to the same illumination images.</p><p>SSIM-MS loss: obtain the image details</p><p>Composite loss: <script type="math/tex">\mathcal{L_{com}}=\alpha \mathcal{L_{enh}}+(1-\alpha)\mathcal{L}_{ssim\_ms},\mathcal{L_{enh}}=\mathcal{L_{L_{1}}}(y,R_x\cdot I_x^{'})</script>  smooth loss and SSIM to keep structure consistency</p></li><li><p>Datasets: CSID, converted from see in the dark(SID) dataset</p></li></ul><h3 id="EnlightenGAN"><a href="#EnlightenGAN" class="headerlink" title="EnlightenGAN"></a>EnlightenGAN</h3><ul><li><p>without paired supervison</p></li><li><p>Global-local discriminator</p><p>Enhance local regions with randomly croped patches</p><p>global loss: LSGAN loss</p></li><li><p>Self feature preserving loss</p><p>Preserve the image content features</p><p><script type="math/tex">\mathcal{L}_{SFP}(I^L)={1\over W_{i,j}H_{i,j}}\sum_{x=1}^{W_{i,j}}\sum_{y=1}^{H_{i,j}}(\phi_{i,j}(I^L)-\phi_{i,j}(G(I^L)))^2</script> , $i$ for max pooling, $j$ for convolutional layer after max pooling</p><p>local discriminator patches also need to be regularized.</p></li><li><p>attention : enhance dart region more effectively</p><p>Question: self labeled attention map?</p></li><li><p>Metrics: NIQE</p></li></ul><h3 id="Attention-guided-low-light-image-enhancement"><a href="#Attention-guided-low-light-image-enhancement" class="headerlink" title="Attention-guided low-light image enhancement"></a>Attention-guided low-light image enhancement</h3><ul><li><p>solve the denosing and low-light enhancement simultaneously</p></li><li><p>Datasets: propose a low-light image simulation pipeline to synthesize realistic low-light images</p></li><li><p>Attention-net: estimate the illumination to guide the attention</p><p>U-Net, retinex-based solution faces difficulties in handling black regions, ue-attention map could solve it.</p></li><li><p>Ehancement-net</p><p>decompose original problem into serveral sub-problems(noise removal, texture preserving, color correction)</p><p>Five different network architectures(why???)</p></li><li><p>Reinforce-net: improve contrast and details</p></li><li><p>Loss: attention: L2. Noise: L1</p><p>Enhancement:</p><p>bright loss: <script type="math/tex">\mathcal{L_{eb}}=||\mathcal{S}(\mathcal{F_e}(I, A', N')-\tilde{I})||^1, \mathcal{S}=\begin{cases} -\lambda x & x <0 \\x&x\ge 0 \end{cases}</script> , ensure sufficient brightness.</p><p>structural loss: SSIM</p><p>perceptual loss</p><p>region loss: balances the degree of enhancement for different regions:<script type="math/tex">\mathcal{L_{er}}=||I'\cdot A'-\tilde{I}\cdot A'||^1+1-\text{ssim}(I'\cdot A',\tilde{I}\cdot A')</script></p><p>$A’$ Is the predicted ue-attention map, $N’$for noise map</p><p>Reinforce:</p><p>bright loss+structural loss + perceptual loss</p></li><li><p>Metrics: PSNR, SSIM, AB, VIF, LOE, TMQI, LPIPS</p></li><li><p>further: blocking artifacts, black regions without any texture, extremely strong noise</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li>Guo, Xiaojie. 《LIME: A Method for Low-light IMage Enhancement》. <em>arXiv:1605.05034 [cs]</em>, 2016年5月17日. <a href="http://arxiv.org/abs/1605.05034">http://arxiv.org/abs/1605.05034</a>.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Methods&quot;&gt;&lt;a href=&quot;#Methods&quot; class=&quot;headerlink&quot; title=&quot;Methods&quot;&gt;&lt;/a&gt;Methods&lt;/h1&gt;&lt;h2 id=&quot;Traditional&quot;&gt;&lt;a href=&quot;#Traditional&quot; class=&quot;he
      
    
    </summary>
    
      <category term="ComputerScience" scheme="http://wiki.rechinx.top/categories/ComputerScience/"/>
    
      <category term="ComputerVision" scheme="http://wiki.rechinx.top/categories/ComputerScience/ComputerVision/"/>
    
    
      <category term="DeepLearning" scheme="http://wiki.rechinx.top/tags/DeepLearning/"/>
    
      <category term="Visions" scheme="http://wiki.rechinx.top/tags/Visions/"/>
    
  </entry>
  
</feed>
